# Механизм детерминированной генерации большого объема данных

## Проблематика

При разработке и тестировании систем обработки данных часто возникает необходимость в создании большого объема тестовых данных. В нашем случае требуется решить следующие проблемы:

- Необходимо создать большой объем записей (сотни миллионов), основанных на профилях пользователей, для эмуляции сырых данных, по которым впоследствии эти профили будут собираться и анализироваться
- Сырые записи содержат как поля, не связанные с профилем (точка продажи, город продажи, канал продажи и т.д.), так и поля, связанные с конкретным профилем (имя, фамилия, телефон, почта, логин и т.д.)
- Требуется возможность управлять частотой появления записей, относящихся к одному и тому же профилю (2, 5, 10 и более записей на один профиль)
- Даже если две сырые записи относятся к одному профилю, связанные с ним поля могут различаться — например, использоваться разные email-адреса из нескольких возможных, имя и фамилия могут быть перепутаны местами, присутствовать опечатки, использоваться транслитерация или данные из разных документов
- Требуется контроль над частотой и видами намеренных искажений данных

Основное ограничение: из-за огромного объема данных нежелательно предварительно генерировать все профили для последующего выбора при создании сырых записей — генерация должна быть максимально быстрой и эффективной.

## Предлагаемое решение

Мы предлагаем использовать механизм идемпотентной (детерминированной) генерации данных профиля на основе идентификатора с вычислительной сложностью O(1).

### Основные принципы

- Вместо предварительной генерации всех профилей, мы генерируем данные профиля "на лету" на основе его уникального идентификатора
- Для каждого идентификатора профиля данные генерируются детерминированно — одинаковый идентификатор всегда дает одинаковые базовые данные профиля
- Мы выделяем разные пулы идентификаторов с заданным статистическим распределением частоты появления записей (например, для 90% профилей генерируется x записей, для 8% — 3x записей, для 2% — 10x записей)
- При генерации сырой записи мы на основе идентификатора или порядкового номера такой записи детерменированно определяем идентификатор профиля и номер варианта сырой записи (на основе чего будет определяться какие элементы массивов из профиля брать, какие искажения вносить)

### Алгоритм работы

1. По индексу записи определяем идентификатор профиля из пула + вариант сырой записи
2. Но основе хеша идентифитора профиля сидируем псевдослучайность для профиля, на основе идентификатора сырой записи сидируем псевдослучайность для сырой записи
3. На основе хеша идентификатора профиля детерминированно генерируем псевдослучайные значения для всех необходимых полей профиля:
    - Индексы в массивах возможных имен (с учетом весовых коэффициентов)
    - Случайные числа в заданных диапазонах
    - Количество вложенных записей (например, число email-адресов от 1 до 5)
    - Сами email-адреса (путем многократного хеширования исходного хеша)
    - Вероятности внесения искажений: перестановка имени и фамилии, транслитерация, опечатки и т.д.
4. На основе хеша идентификатора сырой записи детерминированно генерируем псевдослучайные значения для всех необходимых полей сырой записи, выбор полей из массивов в профиле, порчу полученных данных

### Преимущества подхода

- Высокая производительность — O(1) для генерации одной записи, независимо от общего объема данных
- Отсутствие необходимости хранить предварительно сгенерированные профили
- Воспроизводимость — зная идентификатор сырой записи всегда можно её восстановить
- Гибкое управление частотой появления записей для одного профиля
- Контролируемое внесение искажений в данные для моделирования реальных сценариев
- Возможность генерации данных по временным отрезкам (дни, месяцы, годы)

### Практическое применение

Данный механизм особенно полезен для:

- Тестирования алгоритмов объединения (дедупликации) профилей
- Оценки качества работы систем очистки и нормализации данных
- Проверки корректности обратного преобразования от сырых данных к профилям
- Моделирования реалистичных наборов данных с контролируемым уровнем "шума"

# One filer
